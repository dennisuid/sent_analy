{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchtext\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport torch.nn.utils.rnn as rnn_utils\nimport numpy as np\nimport pandas as pd\nimport random\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom torch.utils.data import random_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-17T16:20:59.475035Z","iopub.execute_input":"2023-04-17T16:20:59.475482Z","iopub.status.idle":"2023-04-17T16:21:04.341322Z","shell.execute_reply.started":"2023-04-17T16:20:59.475422Z","shell.execute_reply":"2023-04-17T16:21:04.339695Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# set random seeds for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nrandom.seed(seed)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:04.343428Z","iopub.execute_input":"2023-04-17T16:21:04.344859Z","iopub.status.idle":"2023-04-17T16:21:04.353580Z","shell.execute_reply.started":"2023-04-17T16:21:04.344815Z","shell.execute_reply":"2023-04-17T16:21:04.352211Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load data\ndf_full = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip', sep='\\t', compression='zip')\n# df_full.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:04.355350Z","iopub.execute_input":"2023-04-17T16:21:04.355757Z","iopub.status.idle":"2023-04-17T16:21:04.666411Z","shell.execute_reply.started":"2023-04-17T16:21:04.355701Z","shell.execute_reply":"2023-04-17T16:21:04.665001Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Tokenize and pad sequences\nvocab = set(\" \".join(df_full[\"Phrase\"]).split())\nvocab.add(\"<PAD>\")\nword_to_ix = {word: i+1 for i, word in enumerate(vocab)}","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:04.668915Z","iopub.execute_input":"2023-04-17T16:21:04.669300Z","iopub.status.idle":"2023-04-17T16:21:04.896325Z","shell.execute_reply.started":"2023-04-17T16:21:04.669264Z","shell.execute_reply":"2023-04-17T16:21:04.894989Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# padding function for each movie review sentence\ndef create_padding(sentence):\n    words = sentence.split()\n    words = words[:10] if len(words) > 10 else words + [\"<PAD>\"] * (10 - len(words))\n    words = ' '.join(words)\n    return words\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:04.898113Z","iopub.execute_input":"2023-04-17T16:21:04.898585Z","iopub.status.idle":"2023-04-17T16:21:04.905122Z","shell.execute_reply.started":"2023-04-17T16:21:04.898549Z","shell.execute_reply":"2023-04-17T16:21:04.903871Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Relevant columns only\n\ndata = df_full.drop(['SentenceId'], axis=1)\ndata = data.iloc[:50000, :]\n\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:04.906976Z","iopub.execute_input":"2023-04-17T16:21:04.907339Z","iopub.status.idle":"2023-04-17T16:21:04.947227Z","shell.execute_reply.started":"2023-04-17T16:21:04.907305Z","shell.execute_reply":"2023-04-17T16:21:04.946017Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   PhraseId   50000 non-null  int64 \n 1   Phrase     50000 non-null  object\n 2   Sentiment  50000 non-null  int64 \ndtypes: int64(2), object(1)\nmemory usage: 1.1+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Remove punctuations\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndata[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: remove_punctuation(text))\n\n# Remove STOPWORDS\n\n\", \".join(stopwords.words('english'))\n\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndata[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: remove_stopwords(text))\n\n# Remove most common words\n\ncnt = Counter()\nfor text in data[\"Phrase\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)\n\nFREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ndata[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: remove_freqwords(text))\n\n# # Remove Stemming \n\n# stemmer = PorterStemmer()\n# def stem_words(text):\n#     return \" \".join([stemmer.stem(word) for word in text.split()])\n\n# data[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: stem_words(text))\n\n# # Lemmatisation\n\n# nltk.download('wordnet')\n\n# lemmatizer = WordNetLemmatizer()\n# wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n# def lemmatize_words(text):\n#     pos_tagged_text = nltk.pos_tag(text.split())\n#     return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\n# data[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: lemmatize_words(text))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:04.948971Z","iopub.execute_input":"2023-04-17T16:21:04.949687Z","iopub.status.idle":"2023-04-17T16:21:05.427004Z","shell.execute_reply.started":"2023-04-17T16:21:04.949648Z","shell.execute_reply":"2023-04-17T16:21:05.425702Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# apply the function to the dataframe column 'Phrase'\n\n# data = data[data['Phrase'].apply(lambda x: len(x.split()) >= 3)]\n\ndata['Phrase'] = data['Phrase'].apply(lambda x: create_padding(x))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:05.428654Z","iopub.execute_input":"2023-04-17T16:21:05.429585Z","iopub.status.idle":"2023-04-17T16:21:05.504391Z","shell.execute_reply.started":"2023-04-17T16:21:05.429545Z","shell.execute_reply":"2023-04-17T16:21:05.502933Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# collate function to provide equal length of tokens in each row of the batch\ndef collate_fn(batch):\n    # Assuming each element of batch is a sequence of tensors\n    # Pad sequences to the same length\n    x_batch, y_batch = zip(*batch)\n    x_batch = rnn_utils.pad_sequence(x_batch, batch_first=True)\n    return x_batch, y_batch","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:05.506081Z","iopub.execute_input":"2023-04-17T16:21:05.506547Z","iopub.status.idle":"2023-04-17T16:21:05.516359Z","shell.execute_reply.started":"2023-04-17T16:21:05.506491Z","shell.execute_reply":"2023-04-17T16:21:05.514899Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Define dataset class\nclass SentimentDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.num_classes = len(set(data[\"Sentiment\"]))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        x = self.data.iloc[index][\"Phrase\"]\n        y = self.data.iloc[index][\"Sentiment\"]\n        x_tokenized = self.tokenizer(x)\n#         print(type(x_tokenized))\n        x_tokenized_len = len(x_tokenized)\n        x_tokenized_ids = [word_to_ix[word] if word in word_to_ix else word_to_ix['<PAD>'] for word in x_tokenized]\n        x_tokenized_tensor = torch.tensor(x_tokenized_ids)\n#         print(f'x_tokenized: {x_tokenized_tensor} and y: {y}')\n        return x_tokenized_tensor, y","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:05.524126Z","iopub.execute_input":"2023-04-17T16:21:05.524561Z","iopub.status.idle":"2023-04-17T16:21:05.537966Z","shell.execute_reply.started":"2023-04-17T16:21:05.524523Z","shell.execute_reply":"2023-04-17T16:21:05.536495Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Define model architecture\nclass SentimentRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        h0 = torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size).to(device)\n        c0 = torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size).to(device)\n        out, _ = self.rnn(x, (h0, c0))\n        out = out[:, -1, :]\n        out = self.fc(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:05.540189Z","iopub.execute_input":"2023-04-17T16:21:05.540705Z","iopub.status.idle":"2023-04-17T16:21:05.552040Z","shell.execute_reply.started":"2023-04-17T16:21:05.540653Z","shell.execute_reply":"2023-04-17T16:21:05.550403Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Define model parameters\ninput_size = len(vocab) + 1\nhidden_size = 128\noutput_size = 5\nnum_layers = 5","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:05.554412Z","iopub.execute_input":"2023-04-17T16:21:05.555587Z","iopub.status.idle":"2023-04-17T16:21:05.562681Z","shell.execute_reply.started":"2023-04-17T16:21:05.555508Z","shell.execute_reply":"2023-04-17T16:21:05.561699Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Rebalancing an unbalanced dataset\n# Find class weights\nclass_counts = data['Sentiment'].value_counts()\nprint(data['Sentiment'].values)\nprint(class_counts)\nclass_weights = 1 / torch.tensor(class_counts, dtype=torch.float)\nprint(class_weights)\n# Compute weights for each data point\nweights = class_weights[data['Sentiment'].values]\nprint(weights[4])\n# Create a sampler for weighted random sampling\nsampler = WeightedRandomSampler(weights, len(weights), replacement=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:05.564518Z","iopub.execute_input":"2023-04-17T16:21:05.564986Z","iopub.status.idle":"2023-04-17T16:21:05.684840Z","shell.execute_reply.started":"2023-04-17T16:21:05.564937Z","shell.execute_reply":"2023-04-17T16:21:05.683889Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[1 2 2 ... 1 4 4]\n2    26581\n3    10423\n1     8214\n4     2746\n0     2036\nName: Sentiment, dtype: int64\ntensor([3.7621e-05, 9.5942e-05, 1.2174e-04, 3.6417e-04, 4.9116e-04])\ntensor(0.0001)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SentimentRNN(input_size, hidden_size, output_size, num_layers).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0005)\nbatch_size = 64\nnum_epochs = 50\ntokenizer = torchtext.data.utils.get_tokenizer('basic_english')\ndataset = SentimentDataset(data, tokenizer)\n\n# Split dataset into train and validation sets\ntrain_size = int(len(dataset) * 0.8)\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\nprint(len(train_dataset))\n\n# Loader with weighted sampling enabled\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, collate_fn=collate_fn, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=sampler, collate_fn=collate_fn, drop_last=True)\n\n# variables to store best loss and best model\nbest_val_loss = float('inf')\nbest_model_path = '/kaggle/working/best_model.pth'\n\n# Train loop\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0    \n    for x_batch, y_batch in train_loader:\n        x_batch = torch.stack(tuple(x_batch)).to(device)\n        y_batch = torch.tensor(y_batch).to(device)\n        optimizer.zero_grad()\n        outputs = model(x_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * x_batch.size(0)\n    epoch_loss = running_loss / len(dataset)\n    # Validation code here\n    val_loss = validate(model, val_loader, criterion)\n    \n    # Save best model based on validation loss\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model_path = 'best_model.pth'\n        torch.save(model.state_dict(), best_model_path)\n        \n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:05:26.987698Z","iopub.execute_input":"2023-04-17T17:05:26.988120Z","iopub.status.idle":"2023-04-17T17:05:27.073315Z","shell.execute_reply.started":"2023-04-17T17:05:26.988085Z","shell.execute_reply":"2023-04-17T17:05:27.071418Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"40000\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_28/3299842287.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}]},{"cell_type":"code","source":"# Load best model:\nbest_model = SentimentRNN()\nbest_model.load_state_dict(torch.load(best_model_path))\n\n# Evaluate model\n\nbest_model.eval() # Set model to evaluation mode\ntest_sentence = \"This movie was terrible. I can not stand this move!\"\ntest_sequence = [word_to_ix[word] if word in word_to_ix else 0 for word in test_sentence.split()[:10]]\ntest_sequence += [0] * (10 - len(test_sequence))\ntest_sequence = torch.tensor(test_sequence, dtype=torch.long).unsqueeze(0).to(device)\nwith torch.no_grad():\n    output = model(test_sequence)\n    prediction = torch.argmax(output, dim=1).item()\nprint(f\"Test Sentence: {test_sentence}\")\nprint(f\"Prediction: {prediction}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:05.969345Z","iopub.status.idle":"2023-04-17T16:21:05.970015Z","shell.execute_reply.started":"2023-04-17T16:21:05.969783Z","shell.execute_reply":"2023-04-17T16:21:05.969810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test file and predictions\n# Load data\ndf_test = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip', sep='\\t', compression='zip')\n# Relevant columns only\ndata_test = df_test.drop(['SentenceId'], axis=1)\n\n# Remove punctuations\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndata_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: remove_punctuation(text))\n\n# Remove STOPWORDS\n\n\", \".join(stopwords.words('english'))\n\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndata_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: remove_stopwords(text))\n\n# Remove most common words\n\ncnt = Counter()\nfor text in data_test[\"Phrase\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)\n\nFREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ndata_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: remove_freqwords(text))\n\n# # Remove Stemming \n\n# stemmer = PorterStemmer()\n# def stem_words(text):\n#     return \" \".join([stemmer.stem(word) for word in text.split()])\n\n# data_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: stem_words(text))\n\n# # Lemmatisation\n\n# nltk.download('wordnet')\n\n# lemmatizer = WordNetLemmatizer()\n# wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n# def lemmatize_words(text):\n#     pos_tagged_text = nltk.pos_tag(text.split())\n#     return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\n# data_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: lemmatize_words(text))\n\ndata_test.info()\n# apply the function to the dataframe column 'Phrase'\n# data_test = data_test.iloc[:75000, :]\ndata_test['Phrase'] = data_test['Phrase'].apply(lambda x: create_padding(x))\n\n# feed into the evaluation model\nBest_model.eval() # Set model to evaluation mode\ny_pred_submission = []\nfor sentence_test in data_test['Phrase']:\n    sentence_test = [word_to_ix[word] if word in word_to_ix else 0 for word in sentence_test.split()[:10]]\n    sentence_test += [0] * (10 - len(sentence_test))\n    sentence_test = torch.tensor(sentence_test, dtype=torch.long).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = model(sentence_test)\n        prediction = torch.argmax(output, dim=1).item()\n        y_pred_submission.append(prediction)\nd = {'PhraseId': data_test['PhraseId'], 'Sentiment': prediction} \nsubmission = pd.DataFrame(data=d)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:05.971935Z","iopub.status.idle":"2023-04-17T16:21:05.972386Z","shell.execute_reply.started":"2023-04-17T16:21:05.972161Z","shell.execute_reply":"2023-04-17T16:21:05.972183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('/kaggle/working/submission.csv', index=False, header=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T16:21:05.973743Z","iopub.status.idle":"2023-04-17T16:21:05.974136Z","shell.execute_reply.started":"2023-04-17T16:21:05.973933Z","shell.execute_reply":"2023-04-17T16:21:05.973952Z"},"trusted":true},"execution_count":null,"outputs":[]}]}