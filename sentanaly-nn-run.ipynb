{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchtext\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport torch.nn.utils.rnn as rnn_utils\nimport numpy as np\nimport pandas as pd\nimport random\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom torch.utils.data import random_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-17T17:38:30.699993Z","iopub.execute_input":"2023-04-17T17:38:30.700662Z","iopub.status.idle":"2023-04-17T17:38:30.708159Z","shell.execute_reply.started":"2023-04-17T17:38:30.700623Z","shell.execute_reply":"2023-04-17T17:38:30.707034Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# set random seeds for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nrandom.seed(seed)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:38:30.710194Z","iopub.execute_input":"2023-04-17T17:38:30.710823Z","iopub.status.idle":"2023-04-17T17:38:30.723391Z","shell.execute_reply.started":"2023-04-17T17:38:30.710783Z","shell.execute_reply":"2023-04-17T17:38:30.722306Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Load data\ndf_full = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip', sep='\\t', compression='zip')\n# df_full.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:38:30.725702Z","iopub.execute_input":"2023-04-17T17:38:30.726483Z","iopub.status.idle":"2023-04-17T17:38:30.908370Z","shell.execute_reply.started":"2023-04-17T17:38:30.726446Z","shell.execute_reply":"2023-04-17T17:38:30.907214Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# padding function for each movie review sentence\ndef create_padding(sentence):\n    words = sentence.split()\n    words = words[:10] if len(words) > 10 else words + [\"<PAD>\"] * (10 - len(words))\n    words = ' '.join(words)\n    return words\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:38:30.909669Z","iopub.execute_input":"2023-04-17T17:38:30.910388Z","iopub.status.idle":"2023-04-17T17:38:30.917078Z","shell.execute_reply.started":"2023-04-17T17:38:30.910348Z","shell.execute_reply":"2023-04-17T17:38:30.915642Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Relevant columns only\n\ndata = df_full.drop(['SentenceId'], axis=1)\ndata = data.iloc[:5000, :]\n\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:38:30.919478Z","iopub.execute_input":"2023-04-17T17:38:30.920126Z","iopub.status.idle":"2023-04-17T17:38:30.940082Z","shell.execute_reply.started":"2023-04-17T17:38:30.920088Z","shell.execute_reply":"2023-04-17T17:38:30.938213Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   PhraseId   5000 non-null   int64 \n 1   Phrase     5000 non-null   object\n 2   Sentiment  5000 non-null   int64 \ndtypes: int64(2), object(1)\nmemory usage: 117.3+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Remove punctuations\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndata[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: remove_punctuation(text))\n\n# Remove STOPWORDS\n\n\", \".join(stopwords.words('english'))\n\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndata[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: remove_stopwords(text))\n\n# Remove most common words\n\ncnt = Counter()\nfor text in data[\"Phrase\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)\n\nFREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ndata[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: remove_freqwords(text))\n\n# # Remove Stemming \n\n# stemmer = PorterStemmer()\n# def stem_words(text):\n#     return \" \".join([stemmer.stem(word) for word in text.split()])\n\n# data[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: stem_words(text))\n\n# # Lemmatisation\n\n# nltk.download('wordnet')\n\n# lemmatizer = WordNetLemmatizer()\n# wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n# def lemmatize_words(text):\n#     pos_tagged_text = nltk.pos_tag(text.split())\n#     return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\n# data[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: lemmatize_words(text))","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:38:30.942887Z","iopub.execute_input":"2023-04-17T17:38:30.943898Z","iopub.status.idle":"2023-04-17T17:38:30.995427Z","shell.execute_reply.started":"2023-04-17T17:38:30.943859Z","shell.execute_reply":"2023-04-17T17:38:30.994512Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# apply the function to the dataframe column 'Phrase'\n\n# data = data[data['Phrase'].apply(lambda x: len(x.split()) >= 3)]\n\ndata['Phrase'] = data['Phrase'].apply(lambda x: create_padding(x))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:38:30.997073Z","iopub.execute_input":"2023-04-17T17:38:30.997479Z","iopub.status.idle":"2023-04-17T17:38:31.008781Z","shell.execute_reply.started":"2023-04-17T17:38:30.997439Z","shell.execute_reply":"2023-04-17T17:38:31.007706Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Tokenize and pad sequences\nvocab = set(\" \".join(df_full[\"Phrase\"]).split())\nvocab.add(\"<PAD>\")\nword_to_ix = {word: i+1 for i, word in enumerate(vocab)}","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:38:31.011494Z","iopub.execute_input":"2023-04-17T17:38:31.012206Z","iopub.status.idle":"2023-04-17T17:38:31.166636Z","shell.execute_reply.started":"2023-04-17T17:38:31.012169Z","shell.execute_reply":"2023-04-17T17:38:31.165330Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# collate function to provide equal length of tokens in each row of the batch\ndef collate_fn(batch):\n    # Assuming each element of batch is a sequence of tensors\n    # Pad sequences to the same length\n    x_batch, y_batch = zip(*batch)\n    x_batch = rnn_utils.pad_sequence(x_batch, batch_first=True)\n    return x_batch, y_batch","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:38:31.168397Z","iopub.execute_input":"2023-04-17T17:38:31.168958Z","iopub.status.idle":"2023-04-17T17:38:31.175353Z","shell.execute_reply.started":"2023-04-17T17:38:31.168919Z","shell.execute_reply":"2023-04-17T17:38:31.174251Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Define dataset class\nclass SentimentDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.num_classes = len(set(data[\"Sentiment\"]))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        x = self.data.iloc[index][\"Phrase\"]\n        y = self.data.iloc[index][\"Sentiment\"]\n        x_tokenized = self.tokenizer(x)\n#         print(type(x_tokenized))\n        x_tokenized_len = len(x_tokenized)\n        x_tokenized_ids = [word_to_ix[word] if word in word_to_ix else word_to_ix['<PAD>'] for word in x_tokenized]\n        x_tokenized_tensor = torch.tensor(x_tokenized_ids)\n#         print(f'x_tokenized: {x_tokenized_tensor} and y: {y}')\n        return x_tokenized_tensor, y","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:38:31.178190Z","iopub.execute_input":"2023-04-17T17:38:31.178656Z","iopub.status.idle":"2023-04-17T17:38:31.188248Z","shell.execute_reply.started":"2023-04-17T17:38:31.178619Z","shell.execute_reply":"2023-04-17T17:38:31.187240Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Define model architecture\nclass SentimentRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        h0 = torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size).to(device)\n        c0 = torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size).to(device)\n        out, _ = self.rnn(x, (h0, c0))\n        out = out[:, -1, :]\n        out = self.fc(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:38:31.189694Z","iopub.execute_input":"2023-04-17T17:38:31.190450Z","iopub.status.idle":"2023-04-17T17:38:31.201475Z","shell.execute_reply.started":"2023-04-17T17:38:31.190406Z","shell.execute_reply":"2023-04-17T17:38:31.200513Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Define model parameters\ninput_size = len(vocab) + 1\nhidden_size = 16\noutput_size = 5\nnum_layers = 3","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:38:31.203747Z","iopub.execute_input":"2023-04-17T17:38:31.204425Z","iopub.status.idle":"2023-04-17T17:38:31.213207Z","shell.execute_reply.started":"2023-04-17T17:38:31.204388Z","shell.execute_reply":"2023-04-17T17:38:31.212166Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Rebalancing an unbalanced dataset\n# Find class weights\nclass_counts = data['Sentiment'].value_counts()\nprint(data['Sentiment'].values)\nprint(class_counts)\nclass_weights = 1 / torch.tensor(class_counts, dtype=torch.float)\nprint(class_weights)\n# Compute weights for each data point\nweights = class_weights[data['Sentiment'].values]\nprint(weights[4])\n# Create a sampler for weighted random sampling\nsampler = WeightedRandomSampler(weights, len(weights), replacement=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:38:31.214882Z","iopub.execute_input":"2023-04-17T17:38:31.215533Z","iopub.status.idle":"2023-04-17T17:38:31.231930Z","shell.execute_reply.started":"2023-04-17T17:38:31.215496Z","shell.execute_reply":"2023-04-17T17:38:31.230547Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"[1 2 2 ... 3 2 2]\n2    2919\n3     898\n1     777\n4     251\n0     155\nName: Sentiment, dtype: int64\ntensor([0.0003, 0.0011, 0.0013, 0.0040, 0.0065])\ntensor(0.0013)\n","output_type":"stream"}]},{"cell_type":"code","source":"def validate(model, data_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    with torch.no_grad():\n        for x_batch, y_batch in data_loader:\n            x_batch = torch.stack(tuple(x_batch)).to(device)\n            y_batch = torch.LongTensor(y_batch).to(device)\n            y_pred = model(x_batch)\n            loss = criterion(y_pred, y_batch)\n            running_loss += loss.item() * x_batch.size(0)\n    return running_loss / len(data_loader.dataset)\n\n# Train model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SentimentRNN(input_size, hidden_size, output_size, num_layers).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.005)\nbatch_size = 64\nnum_epochs = 250\ntokenizer = torchtext.data.utils.get_tokenizer('basic_english')\ndataset = SentimentDataset(data, tokenizer)\n\n# Split dataset into train and validation sets\ntrain_size = int(len(dataset) * 0.8)\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\nprint(len(train_dataset))\n\n# Loader with weighted sampling enabled\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=None, collate_fn=collate_fn, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=None, collate_fn=collate_fn, drop_last=True)\n\n# variables to store best loss and best model\nbest_val_loss = float('inf')\nbest_model_path = '/kaggle/working/best_model.pth'\n\n# Train loop\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0    \n    for x_batch, y_batch in train_loader:\n        x_batch = torch.stack(tuple(x_batch)).to(device)\n        y_batch = torch.tensor(y_batch).to(device)\n        optimizer.zero_grad()\n        outputs = model(x_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * x_batch.size(0)\n    epoch_loss = running_loss / len(dataset)\n    # Validation code here\n    val_loss = validate(model, val_loader, criterion)\n    \n    # Save best model based on validation loss\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model_path = 'best_model.pth'\n        torch.save(model.state_dict(), best_model_path)\n        \n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:38:31.233931Z","iopub.execute_input":"2023-04-17T17:38:31.234313Z","iopub.status.idle":"2023-04-17T17:45:02.808925Z","shell.execute_reply.started":"2023-04-17T17:38:31.234276Z","shell.execute_reply":"2023-04-17T17:45:02.806986Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"4000\nEpoch 1/250, Loss: 0.9458\nEpoch 2/250, Loss: 0.8592\nEpoch 3/250, Loss: 0.8193\nEpoch 4/250, Loss: 0.7788\nEpoch 5/250, Loss: 0.7536\nEpoch 6/250, Loss: 0.7432\nEpoch 7/250, Loss: 0.7208\nEpoch 8/250, Loss: 0.6949\nEpoch 9/250, Loss: 0.6673\nEpoch 10/250, Loss: 0.6526\nEpoch 11/250, Loss: 0.6388\nEpoch 12/250, Loss: 0.6438\nEpoch 13/250, Loss: 0.6321\nEpoch 14/250, Loss: 0.5944\nEpoch 15/250, Loss: 0.5759\nEpoch 16/250, Loss: 0.5541\nEpoch 17/250, Loss: 0.5287\nEpoch 18/250, Loss: 0.5111\nEpoch 19/250, Loss: 0.4837\nEpoch 20/250, Loss: 0.4746\nEpoch 21/250, Loss: 0.4559\nEpoch 22/250, Loss: 0.4464\nEpoch 23/250, Loss: 0.4407\nEpoch 24/250, Loss: 0.4379\nEpoch 25/250, Loss: 0.4319\nEpoch 26/250, Loss: 0.4375\nEpoch 27/250, Loss: 0.4286\nEpoch 28/250, Loss: 0.4260\nEpoch 29/250, Loss: 0.4147\nEpoch 30/250, Loss: 0.4156\nEpoch 31/250, Loss: 0.4061\nEpoch 32/250, Loss: 0.4065\nEpoch 33/250, Loss: 0.4005\nEpoch 34/250, Loss: 0.3901\nEpoch 35/250, Loss: 0.3840\nEpoch 36/250, Loss: 0.3887\nEpoch 37/250, Loss: 0.3846\nEpoch 38/250, Loss: 0.3808\nEpoch 39/250, Loss: 0.3764\nEpoch 40/250, Loss: 0.3770\nEpoch 41/250, Loss: 0.3702\nEpoch 42/250, Loss: 0.3717\nEpoch 43/250, Loss: 0.3627\nEpoch 44/250, Loss: 0.3564\nEpoch 45/250, Loss: 0.3512\nEpoch 46/250, Loss: 0.3540\nEpoch 47/250, Loss: 0.3495\nEpoch 48/250, Loss: 0.3482\nEpoch 49/250, Loss: 0.3449\nEpoch 50/250, Loss: 0.3391\nEpoch 51/250, Loss: 0.3344\nEpoch 52/250, Loss: 0.3278\nEpoch 53/250, Loss: 0.3242\nEpoch 54/250, Loss: 0.3233\nEpoch 55/250, Loss: 0.3211\nEpoch 56/250, Loss: 0.3207\nEpoch 57/250, Loss: 0.3191\nEpoch 58/250, Loss: 0.3161\nEpoch 59/250, Loss: 0.3239\nEpoch 60/250, Loss: 0.3201\nEpoch 61/250, Loss: 0.3250\nEpoch 62/250, Loss: 0.3099\nEpoch 63/250, Loss: 0.3016\nEpoch 64/250, Loss: 0.2995\nEpoch 65/250, Loss: 0.2980\nEpoch 66/250, Loss: 0.2914\nEpoch 67/250, Loss: 0.2918\nEpoch 68/250, Loss: 0.2915\nEpoch 69/250, Loss: 0.2872\nEpoch 70/250, Loss: 0.2894\nEpoch 71/250, Loss: 0.2886\nEpoch 72/250, Loss: 0.2788\nEpoch 73/250, Loss: 0.2777\nEpoch 74/250, Loss: 0.2792\nEpoch 75/250, Loss: 0.2737\nEpoch 76/250, Loss: 0.2818\nEpoch 77/250, Loss: 0.2749\nEpoch 78/250, Loss: 0.2890\nEpoch 79/250, Loss: 0.2907\nEpoch 80/250, Loss: 0.2763\nEpoch 81/250, Loss: 0.2663\nEpoch 82/250, Loss: 0.2593\nEpoch 83/250, Loss: 0.2576\nEpoch 84/250, Loss: 0.2541\nEpoch 85/250, Loss: 0.2582\nEpoch 86/250, Loss: 0.2537\nEpoch 87/250, Loss: 0.2520\nEpoch 88/250, Loss: 0.2523\nEpoch 89/250, Loss: 0.2477\nEpoch 90/250, Loss: 0.2515\nEpoch 91/250, Loss: 0.2399\nEpoch 92/250, Loss: 0.2497\nEpoch 93/250, Loss: 0.2405\nEpoch 94/250, Loss: 0.2470\nEpoch 95/250, Loss: 0.2523\nEpoch 96/250, Loss: 0.2412\nEpoch 97/250, Loss: 0.2364\nEpoch 98/250, Loss: 0.2285\nEpoch 99/250, Loss: 0.2275\nEpoch 100/250, Loss: 0.2245\nEpoch 101/250, Loss: 0.2283\nEpoch 102/250, Loss: 0.2250\nEpoch 103/250, Loss: 0.2275\nEpoch 104/250, Loss: 0.2260\nEpoch 105/250, Loss: 0.2218\nEpoch 106/250, Loss: 0.2231\nEpoch 107/250, Loss: 0.2180\nEpoch 108/250, Loss: 0.2147\nEpoch 109/250, Loss: 0.2113\nEpoch 110/250, Loss: 0.2098\nEpoch 111/250, Loss: 0.2078\nEpoch 112/250, Loss: 0.2084\nEpoch 113/250, Loss: 0.2055\nEpoch 114/250, Loss: 0.2089\nEpoch 115/250, Loss: 0.2128\nEpoch 116/250, Loss: 0.2213\nEpoch 117/250, Loss: 0.2211\nEpoch 118/250, Loss: 0.2289\nEpoch 119/250, Loss: 0.2399\nEpoch 120/250, Loss: 0.2376\nEpoch 121/250, Loss: 0.2317\nEpoch 122/250, Loss: 0.2247\nEpoch 123/250, Loss: 0.2108\nEpoch 124/250, Loss: 0.2092\nEpoch 125/250, Loss: 0.2037\nEpoch 126/250, Loss: 0.2028\nEpoch 127/250, Loss: 0.2006\nEpoch 128/250, Loss: 0.2000\nEpoch 129/250, Loss: 0.1988\nEpoch 130/250, Loss: 0.1983\nEpoch 131/250, Loss: 0.2028\nEpoch 132/250, Loss: 0.2074\nEpoch 133/250, Loss: 0.2072\nEpoch 134/250, Loss: 0.2111\nEpoch 135/250, Loss: 0.2162\nEpoch 136/250, Loss: 0.2201\nEpoch 137/250, Loss: 0.2270\nEpoch 138/250, Loss: 0.2207\nEpoch 139/250, Loss: 0.2153\nEpoch 140/250, Loss: 0.2170\nEpoch 141/250, Loss: 0.2127\nEpoch 142/250, Loss: 0.2171\nEpoch 143/250, Loss: 0.2195\nEpoch 144/250, Loss: 0.2201\nEpoch 145/250, Loss: 0.2181\nEpoch 146/250, Loss: 0.2057\nEpoch 147/250, Loss: 0.2010\nEpoch 148/250, Loss: 0.1975\nEpoch 149/250, Loss: 0.1989\nEpoch 150/250, Loss: 0.1980\nEpoch 151/250, Loss: 0.1934\nEpoch 152/250, Loss: 0.1922\nEpoch 153/250, Loss: 0.1913\nEpoch 154/250, Loss: 0.1890\nEpoch 155/250, Loss: 0.1879\nEpoch 156/250, Loss: 0.1894\nEpoch 157/250, Loss: 0.1888\nEpoch 158/250, Loss: 0.1903\nEpoch 159/250, Loss: 0.1910\nEpoch 160/250, Loss: 0.1934\nEpoch 161/250, Loss: 0.2040\nEpoch 162/250, Loss: 0.2041\nEpoch 163/250, Loss: 0.2034\nEpoch 164/250, Loss: 0.1978\nEpoch 165/250, Loss: 0.1996\nEpoch 166/250, Loss: 0.1961\nEpoch 167/250, Loss: 0.1979\nEpoch 168/250, Loss: 0.2076\nEpoch 169/250, Loss: 0.2027\nEpoch 170/250, Loss: 0.2047\nEpoch 171/250, Loss: 0.2133\nEpoch 172/250, Loss: 0.2086\nEpoch 173/250, Loss: 0.2090\nEpoch 174/250, Loss: 0.2074\nEpoch 175/250, Loss: 0.1984\nEpoch 176/250, Loss: 0.1958\nEpoch 177/250, Loss: 0.2013\nEpoch 178/250, Loss: 0.1959\nEpoch 179/250, Loss: 0.1917\nEpoch 180/250, Loss: 0.1914\nEpoch 181/250, Loss: 0.1990\nEpoch 182/250, Loss: 0.2038\nEpoch 183/250, Loss: 0.1995\nEpoch 184/250, Loss: 0.1980\nEpoch 185/250, Loss: 0.1951\nEpoch 186/250, Loss: 0.1939\nEpoch 187/250, Loss: 0.1964\nEpoch 188/250, Loss: 0.1948\nEpoch 189/250, Loss: 0.1931\nEpoch 190/250, Loss: 0.1883\nEpoch 191/250, Loss: 0.1857\nEpoch 192/250, Loss: 0.1845\nEpoch 193/250, Loss: 0.1833\nEpoch 194/250, Loss: 0.1826\nEpoch 195/250, Loss: 0.1820\nEpoch 196/250, Loss: 0.1818\nEpoch 197/250, Loss: 0.1817\nEpoch 198/250, Loss: 0.1838\nEpoch 199/250, Loss: 0.1813\nEpoch 200/250, Loss: 0.1809\nEpoch 201/250, Loss: 0.1806\nEpoch 202/250, Loss: 0.1806\nEpoch 203/250, Loss: 0.1801\nEpoch 204/250, Loss: 0.1802\nEpoch 205/250, Loss: 0.1822\nEpoch 206/250, Loss: 0.1831\nEpoch 207/250, Loss: 0.1887\nEpoch 208/250, Loss: 0.2277\nEpoch 209/250, Loss: 0.2665\nEpoch 210/250, Loss: 0.2630\nEpoch 211/250, Loss: 0.2534\nEpoch 212/250, Loss: 0.2405\nEpoch 213/250, Loss: 0.2241\nEpoch 214/250, Loss: 0.2081\nEpoch 215/250, Loss: 0.2062\nEpoch 216/250, Loss: 0.1943\nEpoch 217/250, Loss: 0.1912\nEpoch 218/250, Loss: 0.1862\nEpoch 219/250, Loss: 0.1853\nEpoch 220/250, Loss: 0.1834\nEpoch 221/250, Loss: 0.1821\nEpoch 222/250, Loss: 0.1825\nEpoch 223/250, Loss: 0.1827\nEpoch 224/250, Loss: 0.1818\nEpoch 225/250, Loss: 0.1820\nEpoch 226/250, Loss: 0.1843\nEpoch 227/250, Loss: 0.1818\nEpoch 228/250, Loss: 0.1812\nEpoch 229/250, Loss: 0.1838\nEpoch 230/250, Loss: 0.1859\nEpoch 231/250, Loss: 0.1845\nEpoch 232/250, Loss: 0.1866\nEpoch 233/250, Loss: 0.1868\nEpoch 234/250, Loss: 0.1925\nEpoch 235/250, Loss: 0.1921\nEpoch 236/250, Loss: 0.1975\nEpoch 237/250, Loss: 0.2065\nEpoch 238/250, Loss: 0.2104\nEpoch 239/250, Loss: 0.2063\nEpoch 240/250, Loss: 0.1982\nEpoch 241/250, Loss: 0.2016\nEpoch 242/250, Loss: 0.1936\nEpoch 243/250, Loss: 0.1920\nEpoch 244/250, Loss: 0.1847\nEpoch 245/250, Loss: 0.1844\nEpoch 246/250, Loss: 0.1833\nEpoch 247/250, Loss: 0.1848\nEpoch 248/250, Loss: 0.1846\nEpoch 249/250, Loss: 0.1850\nEpoch 250/250, Loss: 0.1875\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load best model:\nbest_model = SentimentRNN(input_size, hidden_size, output_size, num_layers).to(device)\nbest_model.load_state_dict(torch.load(best_model_path))\n\n# Evaluate model\n\nbest_model.eval() # Set model to evaluation mode\ntest_sentence = \"This movie was terrible. I can not stand this move!\"\ntest_sequence = [word_to_ix[word] if word in word_to_ix else 0 for word in test_sentence.split()[:10]]\ntest_sequence += [0] * (10 - len(test_sequence))\ntest_sequence = torch.tensor(test_sequence, dtype=torch.long).unsqueeze(0).to(device)\nwith torch.no_grad():\n    output = model(test_sequence)\n    prediction = torch.argmax(output, dim=1).item()\nprint(f\"Test Sentence: {test_sentence}\")\nprint(f\"Prediction: {prediction}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:45:02.811955Z","iopub.execute_input":"2023-04-17T17:45:02.812320Z","iopub.status.idle":"2023-04-17T17:45:02.917480Z","shell.execute_reply.started":"2023-04-17T17:45:02.812290Z","shell.execute_reply":"2023-04-17T17:45:02.916291Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Test Sentence: This movie was terrible. I can not stand this move!\nPrediction: 3\n","output_type":"stream"}]},{"cell_type":"code","source":"# test file and predictions\n# Load data\ndf_test = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip', sep='\\t', compression='zip')\n# Relevant columns only\ndata_test = df_test.drop(['SentenceId'], axis=1)\n\n# Remove punctuations\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndata_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: remove_punctuation(text))\n\n# Remove STOPWORDS\n\n\", \".join(stopwords.words('english'))\n\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndata_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: remove_stopwords(text))\n\n# Remove most common words\n\ncnt = Counter()\nfor text in data_test[\"Phrase\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)\n\nFREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ndata_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: remove_freqwords(text))\n\n# # Remove Stemming \n\n# stemmer = PorterStemmer()\n# def stem_words(text):\n#     return \" \".join([stemmer.stem(word) for word in text.split()])\n\n# data_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: stem_words(text))\n\n# # Lemmatisation\n\n# nltk.download('wordnet')\n\n# lemmatizer = WordNetLemmatizer()\n# wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n# def lemmatize_words(text):\n#     pos_tagged_text = nltk.pos_tag(text.split())\n#     return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\n# data_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: lemmatize_words(text))\n\ndata_test.info()\n# apply the function to the dataframe column 'Phrase'\n# data_test = data_test.iloc[:10000, :]\ndata_test['Phrase'] = data_test['Phrase'].apply(lambda x: create_padding(x))\n\n# feed into the evaluation model\nbest_model.eval() # Set model to evaluation mode\ny_pred_submission = []\nfor sentence_test in data_test['Phrase']:\n    sentence_test = [word_to_ix[word] if word in word_to_ix else 0 for word in sentence_test.split()[:10]]\n    sentence_test += [0] * (10 - len(sentence_test))\n    sentence_test = torch.tensor(sentence_test, dtype=torch.long).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = model(sentence_test)\n        prediction = torch.argmax(output, dim=1).item()\n        y_pred_submission.append(prediction)\nd = {'PhraseId': data_test['PhraseId'], 'Sentiment': prediction} \nsubmission = pd.DataFrame(data=d)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:45:02.919414Z","iopub.execute_input":"2023-04-17T17:45:02.919818Z","iopub.status.idle":"2023-04-17T17:45:52.390961Z","shell.execute_reply.started":"2023-04-17T17:45:02.919780Z","shell.execute_reply":"2023-04-17T17:45:52.389772Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 66292 entries, 0 to 66291\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   PhraseId  66292 non-null  int64 \n 1   Phrase    66292 non-null  object\ndtypes: int64(1), object(1)\nmemory usage: 1.0+ MB\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"       PhraseId  Sentiment\n0        156061          0\n1        156062          0\n2        156063          0\n3        156064          0\n4        156065          0\n...         ...        ...\n66287    222348          0\n66288    222349          0\n66289    222350          0\n66290    222351          0\n66291    222352          0\n\n[66292 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>156061</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>156062</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>156063</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>156064</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>156065</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>66287</th>\n      <td>222348</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>66288</th>\n      <td>222349</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>66289</th>\n      <td>222350</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>66290</th>\n      <td>222351</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>66291</th>\n      <td>222352</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>66292 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission.to_csv('/kaggle/working/submission.csv', index=False, header=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-17T17:45:52.392594Z","iopub.execute_input":"2023-04-17T17:45:52.392998Z","iopub.status.idle":"2023-04-17T17:45:52.458447Z","shell.execute_reply.started":"2023-04-17T17:45:52.392959Z","shell.execute_reply":"2023-04-17T17:45:52.457408Z"},"trusted":true},"execution_count":31,"outputs":[]}]}