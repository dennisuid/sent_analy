{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchtext\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport torch.nn.utils.rnn as rnn_utils\nimport numpy as np\nimport pandas as pd\nimport random\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom torch.utils.data import random_split\nimport os\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-21T18:02:24.326534Z","iopub.execute_input":"2023-04-21T18:02:24.327221Z","iopub.status.idle":"2023-04-21T18:02:28.603406Z","shell.execute_reply.started":"2023-04-21T18:02:24.327184Z","shell.execute_reply":"2023-04-21T18:02:28.602197Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# set random seeds for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nrandom.seed(seed)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:02:28.608446Z","iopub.execute_input":"2023-04-21T18:02:28.609080Z","iopub.status.idle":"2023-04-21T18:02:28.620607Z","shell.execute_reply.started":"2023-04-21T18:02:28.609039Z","shell.execute_reply":"2023-04-21T18:02:28.619245Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Load data\ndf_full = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/train.tsv.zip', sep='\\t', compression='zip')\n# df_full.info()\n\n# # shuffle the rows\n# df_full = df_full.sample(frac=1, random_state=42)\n\n# # reset the index after shuffling\n# df_full = df_full.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:02:28.627025Z","iopub.execute_input":"2023-04-21T18:02:28.627496Z","iopub.status.idle":"2023-04-21T18:02:28.941913Z","shell.execute_reply.started":"2023-04-21T18:02:28.627458Z","shell.execute_reply":"2023-04-21T18:02:28.940834Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# padding function for each movie review sentence\ndef create_padding(sentence):\n    words = sentence.split()\n    words = words[:30] if len(words) > 30 else words + [\"<PAD>\"] * (30 - len(words))\n    words = ' '.join(words)\n    return words\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:02:28.943359Z","iopub.execute_input":"2023-04-21T18:02:28.943943Z","iopub.status.idle":"2023-04-21T18:02:28.950352Z","shell.execute_reply.started":"2023-04-21T18:02:28.943903Z","shell.execute_reply":"2023-04-21T18:02:28.949638Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Relevant columns only\n\ndata = df_full.drop(['SentenceId'], axis=1)\ndata = data.iloc[:50000, :]\n\ndata.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:02:28.952007Z","iopub.execute_input":"2023-04-21T18:02:28.952772Z","iopub.status.idle":"2023-04-21T18:02:28.989683Z","shell.execute_reply.started":"2023-04-21T18:02:28.952735Z","shell.execute_reply":"2023-04-21T18:02:28.988605Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 50000 entries, 0 to 49999\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype \n---  ------     --------------  ----- \n 0   PhraseId   50000 non-null  int64 \n 1   Phrase     50000 non-null  object\n 2   Sentiment  50000 non-null  int64 \ndtypes: int64(2), object(1)\nmemory usage: 1.1+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# Remove punctuations\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndata[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: remove_punctuation(text))\n\n# Remove STOPWORDS\n\n\", \".join(stopwords.words('english'))\n\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ndata[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: remove_stopwords(text))\n\n# Remove most common words\n\ncnt = Counter()\nfor text in data[\"Phrase\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)\n\nFREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ndata[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: remove_freqwords(text))\n\n# # Remove Stemming \n\n# stemmer = PorterStemmer()\n# def stem_words(text):\n#     return \" \".join([stemmer.stem(word) for word in text.split()])\n\n# data[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: stem_words(text))\n\n# # Lemmatisation\n\n# nltk.download('wordnet')\n\n# lemmatizer = WordNetLemmatizer()\n# wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n# def lemmatize_words(text):\n#     pos_tagged_text = nltk.pos_tag(text.split())\n#     return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\n# data[\"Phrase\"] = data[\"Phrase\"].apply(lambda text: lemmatize_words(text))","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:02:28.991147Z","iopub.execute_input":"2023-04-21T18:02:28.991735Z","iopub.status.idle":"2023-04-21T18:02:29.586267Z","shell.execute_reply.started":"2023-04-21T18:02:28.991697Z","shell.execute_reply":"2023-04-21T18:02:29.585123Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# apply the function to the dataframe column 'Phrase'\n\n# data = data[data['Phrase'].apply(lambda x: len(x.split()) >= 3)]\n\ndata['Phrase'] = data['Phrase'].apply(lambda x: create_padding(x))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:02:29.587857Z","iopub.execute_input":"2023-04-21T18:02:29.588202Z","iopub.status.idle":"2023-04-21T18:02:29.707160Z","shell.execute_reply.started":"2023-04-21T18:02:29.588166Z","shell.execute_reply":"2023-04-21T18:02:29.705447Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Tokenize and pad sequences\nvocab = set(\" \".join(data[\"Phrase\"]).split())\nvocab.add(\"<PAD>\")\nword_to_ix = {word: i+1 for i, word in enumerate(vocab)}","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:02:29.708594Z","iopub.execute_input":"2023-04-21T18:02:29.709885Z","iopub.status.idle":"2023-04-21T18:02:29.948117Z","shell.execute_reply.started":"2023-04-21T18:02:29.709844Z","shell.execute_reply":"2023-04-21T18:02:29.946900Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# collate function to provide equal length of tokens in each row of the batch\ndef collate_fn(batch):\n    # Assuming each element of batch is a sequence of tensors\n    # Pad sequences to the same length\n    x_batch, y_batch = zip(*batch)\n    x_batch = rnn_utils.pad_sequence(x_batch, batch_first=True)\n    return x_batch, y_batch","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:02:29.952866Z","iopub.execute_input":"2023-04-21T18:02:29.953613Z","iopub.status.idle":"2023-04-21T18:02:29.959462Z","shell.execute_reply.started":"2023-04-21T18:02:29.953569Z","shell.execute_reply":"2023-04-21T18:02:29.958387Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Define dataset class\nclass SentimentDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.num_classes = len(set(data[\"Sentiment\"]))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        x = self.data.iloc[index][\"Phrase\"]\n        y = self.data.iloc[index][\"Sentiment\"]\n        x_tokenized = self.tokenizer(x)\n        x_tokenized_len = len(x_tokenized)\n        x_tokenized_ids = [word_to_ix[word] if word in word_to_ix else word_to_ix['<PAD>'] for word in x_tokenized]\n        x_tokenized_tensor = torch.tensor(x_tokenized_ids)\n        return x_tokenized_tensor, y","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:02:29.961477Z","iopub.execute_input":"2023-04-21T18:02:29.961764Z","iopub.status.idle":"2023-04-21T18:02:29.972678Z","shell.execute_reply.started":"2023-04-21T18:02:29.961708Z","shell.execute_reply":"2023-04-21T18:02:29.971697Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Define model architecture\n\nclass SentimentRNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size, num_layers, embedding_dim=4, dropout_prob=0.3):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout_prob = dropout_prob\n        \n        self.embedding = nn.Embedding(input_size, embedding_dim)\n        self.rnn = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n        self.fc = nn.Linear(hidden_size, output_size)\n        self.dropout = nn.Dropout(dropout_prob)\n        \n    def forward(self, x):\n        batch_size = x.size(0)\n        hidden = self.init_hidden(batch_size)\n        embeds = self.embedding(x)\n        rnn_out, hidden = self.rnn(embeds, hidden)\n        rnn_out = self.dropout(rnn_out)\n        out = self.fc(rnn_out[:, -1, :])\n        return out\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_().to(device),\n                  weight.new(self.num_layers, batch_size, self.hidden_size).zero_().to(device))\n        return hidden\n","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:02:29.976243Z","iopub.execute_input":"2023-04-21T18:02:29.976576Z","iopub.status.idle":"2023-04-21T18:02:29.987492Z","shell.execute_reply.started":"2023-04-21T18:02:29.976550Z","shell.execute_reply":"2023-04-21T18:02:29.986400Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Define model parameters\ninput_size = len(vocab) + 1\nhidden_size = 8\noutput_size = 5\nnum_layers = 2","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:02:29.988816Z","iopub.execute_input":"2023-04-21T18:02:29.989929Z","iopub.status.idle":"2023-04-21T18:02:29.997248Z","shell.execute_reply.started":"2023-04-21T18:02:29.989894Z","shell.execute_reply":"2023-04-21T18:02:29.996192Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# # Rebalancing an unbalanced dataset\n# # Find class weights\n# class_counts = data['Sentiment'].value_counts()\n# print(data['Sentiment'].values)\n# print(class_counts)\n# class_weights = 1 / torch.tensor(class_counts, dtype=torch.float)\n# print(class_weights)\n# # Compute weights for each data point\n# weights = class_weights[data['Sentiment'].values]\n# print(weights[4])\n# # Create a sampler for weighted random sampling\n# sampler = WeightedRandomSampler(weights, len(weights), replacement=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:02:30.000577Z","iopub.execute_input":"2023-04-21T18:02:30.000839Z","iopub.status.idle":"2023-04-21T18:02:30.007977Z","shell.execute_reply.started":"2023-04-21T18:02:30.000814Z","shell.execute_reply":"2023-04-21T18:02:30.006998Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def validate(model, data_loader, criterion):\n    model.eval()\n    running_loss = 0.0\n    correct_preds = 0\n    total_preds = 0\n    with torch.no_grad():\n        for x_batch, y_batch in data_loader:\n            x_batch = torch.stack(tuple(x_batch)).to(device)\n            y_batch = torch.LongTensor(y_batch).to(device)\n            y_pred = model(x_batch)\n            loss = criterion(y_pred, y_batch)\n            running_loss += loss.item() * x_batch.size(0)\n            _, predicted = torch.max(y_pred, 1)\n            correct_preds += (predicted == y_batch).sum().item()\n            total_preds += x_batch.size(0)\n    val_loss = running_loss / len(data_loader.dataset)\n    val_accuracy = correct_preds / total_preds\n    return val_loss, val_accuracy\n\n\n# Train model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = SentimentRNN(input_size, hidden_size, output_size, num_layers).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\nbatch_size = 256\nnum_epochs = 250\ntokenizer = torchtext.data.utils.get_tokenizer('basic_english')\ndataset = SentimentDataset(data, tokenizer)\n\n# Split dataset into train and validation sets\ntrain_size = int(len(dataset) * 0.8)\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\nprint(len(train_dataset))\n\n# Loader with weighted sampling enabled\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=None, collate_fn=collate_fn, drop_last=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=None, collate_fn=collate_fn, drop_last=True)\n\n# variables to store best loss and best model\nbest_val_loss = float('inf')\nbest_model_path = '/kaggle/working/best_model.pth'\n\n# Load best model:\nif os.path.exists(best_model_path):\n    model.load_state_dict(torch.load(best_model_path))\n\n# Create a learning rate scheduler\nscheduler = ReduceLROnPlateau(optimizer, factor=0.99, patience=4)\n\n# Train loop\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0    \n    for x_batch, y_batch in train_loader:\n        x_batch = torch.stack(tuple(x_batch)).to(device)\n        y_batch = torch.tensor(y_batch).to(device)\n        optimizer.zero_grad()\n        outputs = model(x_batch)\n        loss = criterion(outputs, y_batch)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * x_batch.size(0)\n    epoch_loss = running_loss / len(dataset)\n    # Validation code here\n    val_loss, val_accuracy = validate(model, val_loader, criterion)\n    \n    # Save best model based on validation loss\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model_path = 'best_model.pth'\n        torch.save(model.state_dict(), best_model_path)\n    \n    # Retrieve the current learning rate\n    lr = optimizer.param_groups[0]['lr']\n    \n    # Update learning rate\n    scheduler.step(val_loss)\n        \n    print(f\"Epoch {epoch+1}/{num_epochs}, Train_loss: {epoch_loss:.6f}, Val_loss: {val_loss:.6f}, Val_accu: {val_accuracy:.6f}, LR: {lr:.6f}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:02:30.009546Z","iopub.execute_input":"2023-04-21T18:02:30.010316Z","iopub.status.idle":"2023-04-21T18:06:30.013685Z","shell.execute_reply.started":"2023-04-21T18:02:30.010260Z","shell.execute_reply":"2023-04-21T18:06:30.010369Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"40000\nEpoch 1/250, Train_loss: 1.021418, Val_loss: 1.233258, Val_accu: 0.539163, LR: 0.010000\nEpoch 2/250, Train_loss: 1.005634, Val_loss: 1.232971, Val_accu: 0.539163, LR: 0.010000\nEpoch 3/250, Train_loss: 1.003300, Val_loss: 1.232619, Val_accu: 0.539163, LR: 0.010000\nEpoch 4/250, Train_loss: 1.002118, Val_loss: 1.232727, Val_accu: 0.539163, LR: 0.010000\nEpoch 5/250, Train_loss: 1.001374, Val_loss: 1.232636, Val_accu: 0.539163, LR: 0.010000\nEpoch 6/250, Train_loss: 1.001323, Val_loss: 1.232709, Val_accu: 0.539163, LR: 0.010000\nEpoch 7/250, Train_loss: 1.000921, Val_loss: 1.232627, Val_accu: 0.539163, LR: 0.010000\nEpoch 8/250, Train_loss: 1.001018, Val_loss: 1.232624, Val_accu: 0.539163, LR: 0.010000\nEpoch 9/250, Train_loss: 1.000953, Val_loss: 1.232670, Val_accu: 0.539163, LR: 0.009900\nEpoch 10/250, Train_loss: 1.000827, Val_loss: 1.232557, Val_accu: 0.539163, LR: 0.009900\nEpoch 11/250, Train_loss: 1.000908, Val_loss: 1.232582, Val_accu: 0.539163, LR: 0.009900\nEpoch 12/250, Train_loss: 1.000844, Val_loss: 1.232573, Val_accu: 0.539163, LR: 0.009900\nEpoch 13/250, Train_loss: 1.000786, Val_loss: 1.232580, Val_accu: 0.539163, LR: 0.009900\nEpoch 14/250, Train_loss: 1.000838, Val_loss: 1.232582, Val_accu: 0.539163, LR: 0.009801\nEpoch 15/250, Train_loss: 1.000789, Val_loss: 1.232549, Val_accu: 0.539163, LR: 0.009801\nEpoch 16/250, Train_loss: 1.000794, Val_loss: 1.232559, Val_accu: 0.539163, LR: 0.009801\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_24/2279198.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_24/1132450851.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Phrase\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sentiment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mx_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx_tokenized_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1549\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mis_bool_indexer\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mconvert\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     if isinstance(key, (ABCSeries, np.ndarray, ABCIndex)) or (\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mis_array_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     ):\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/dtypes/generic.py\u001b[0m in \u001b[0;36m_check\u001b[0;34m(cls, inst)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_typ\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mdct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"__instancecheck__\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__subclasscheck__\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_check\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Load best model\nbest_model = SentimentRNN(input_size, hidden_size, output_size, num_layers).to(device)\nbest_model.load_state_dict(torch.load(best_model_path))\n\n# Evaluate model\n\nbest_model.eval() # Set model to evaluation mode\ntest_sentence = \"This movie was terrible. I can not stand this move!\"\ntest_sequence = [word_to_ix[word] if word in word_to_ix else 0 for word in test_sentence.split()[:30]]\ntest_sequence += [0] * (30 - len(test_sequence))\ntest_sequence = torch.tensor(test_sequence, dtype=torch.long).unsqueeze(0).to(device)\nwith torch.no_grad():\n    output = model(test_sequence)\n    prediction = torch.argmax(output, dim=1).item()\nprint(f\"Test Sentence: {test_sentence}\")\nprint(f\"Prediction: {prediction}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:06:36.090387Z","iopub.execute_input":"2023-04-21T18:06:36.091104Z","iopub.status.idle":"2023-04-21T18:06:36.116085Z","shell.execute_reply.started":"2023-04-21T18:06:36.091067Z","shell.execute_reply":"2023-04-21T18:06:36.115009Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Test Sentence: This movie was terrible. I can not stand this move!\nPrediction: 2\n","output_type":"stream"}]},{"cell_type":"code","source":"# test file and predictions\n# Load data\ndf_test = pd.read_csv('/kaggle/input/sentiment-analysis-on-movie-reviews/test.tsv.zip', sep='\\t', compression='zip')\n# Relevant columns only\ndata_test = df_test.drop(['SentenceId'], axis=1)\n\n# Remove punctuations\n\ndata_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: remove_punctuation(text))\n\n# Remove STOPWORDS\n\ndata_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: remove_stopwords(text))\n\n# Remove most common words\n\ncnt = Counter()\nfor text in data_test[\"Phrase\"].values:\n    for word in text.split():\n        cnt[word] += 1\n        \ncnt.most_common(10)\n\nFREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\ndef remove_freqwords(text):\n    \"\"\"custom function to remove the frequent words\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\n\ndata_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: remove_freqwords(text))\n\n# Remove Stemming \n\nstemmer = PorterStemmer()\ndef stem_words(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\ndata_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: stem_words(text))\n\n# # Lemmatisation\n\n# nltk.download('wordnet')\n\n# lemmatizer = WordNetLemmatizer()\n# wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n# def lemmatize_words(text):\n#     pos_tagged_text = nltk.pos_tag(text.split())\n#     return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\n# data_test[\"Phrase\"] = data_test[\"Phrase\"].apply(lambda text: lemmatize_words(text))\n\ndata_test.info()\n# apply the function to the dataframe column 'Phrase'\n# data_test = data_test.iloc[:10000, :]\ndata_test['Phrase'] = data_test['Phrase'].apply(lambda x: create_padding(x))\n\n# feed into the evaluation model\nbest_model.eval() # Set model to evaluation mode\ny_pred_submission = []\nfor sentence_test in data_test['Phrase']:\n    sentence_test = [word_to_ix[word] if word in word_to_ix else 0 for word in sentence_test.split()[:30]]\n    sentence_test += [0] * (30 - len(sentence_test))\n    sentence_test = torch.tensor(sentence_test, dtype=torch.long).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = model(sentence_test)\n        prediction = torch.argmax(output, dim=1).item()\n        y_pred_submission.append(prediction)\nd = {'PhraseId': data_test['PhraseId'], 'Sentiment': prediction} \nsubmission = pd.DataFrame(data=d)\nsubmission","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:06:39.324043Z","iopub.execute_input":"2023-04-21T18:06:39.324512Z","iopub.status.idle":"2023-04-21T18:07:50.085271Z","shell.execute_reply.started":"2023-04-21T18:06:39.324470Z","shell.execute_reply":"2023-04-21T18:07:50.084319Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 66292 entries, 0 to 66291\nData columns (total 2 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   PhraseId  66292 non-null  int64 \n 1   Phrase    66292 non-null  object\ndtypes: int64(1), object(1)\nmemory usage: 1.0+ MB\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"       PhraseId  Sentiment\n0        156061          2\n1        156062          2\n2        156063          2\n3        156064          2\n4        156065          2\n...         ...        ...\n66287    222348          2\n66288    222349          2\n66289    222350          2\n66290    222351          2\n66291    222352          2\n\n[66292 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PhraseId</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>156061</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>156062</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>156063</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>156064</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>156065</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>66287</th>\n      <td>222348</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>66288</th>\n      <td>222349</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>66289</th>\n      <td>222350</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>66290</th>\n      <td>222351</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>66291</th>\n      <td>222352</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>66292 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission.to_csv('/kaggle/working/submission.csv', index=False, header=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-21T18:08:04.530454Z","iopub.execute_input":"2023-04-21T18:08:04.531386Z","iopub.status.idle":"2023-04-21T18:08:04.592844Z","shell.execute_reply.started":"2023-04-21T18:08:04.531347Z","shell.execute_reply":"2023-04-21T18:08:04.591919Z"},"trusted":true},"execution_count":17,"outputs":[]}]}